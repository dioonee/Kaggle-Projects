{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-05-14T11:09:13.550573Z",
     "iopub.status.busy": "2021-05-14T11:09:13.549468Z",
     "iopub.status.idle": "2021-05-14T11:09:14.598712Z",
     "shell.execute_reply": "2021-05-14T11:09:14.599212Z"
    },
    "papermill": {
     "duration": 1.072343,
     "end_time": "2021-05-14T11:09:14.599519",
     "exception": false,
     "start_time": "2021-05-14T11:09:13.527176",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample_submission.csv', 'test', 'train']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import torch\n",
    "import os\n",
    "print(os.listdir('./snu-2021-1-ds-project-3'))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:09:14.641744Z",
     "iopub.status.busy": "2021-05-14T11:09:14.640774Z",
     "iopub.status.idle": "2021-05-14T11:09:14.643501Z",
     "shell.execute_reply": "2021-05-14T11:09:14.643982Z"
    },
    "papermill": {
     "duration": 0.025616,
     "end_time": "2021-05-14T11:09:14.644158",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.618542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_dir = './snu-2021-1-ds-project-3/train'\n",
    "test_dir = './snu-2021-1-ds-project-3/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_record_df = pd.read_csv('./data/train_record_df.csv', index_col=0)\n",
    "train_attrib_df = pd.read_csv('./data/train_attrib_df.csv', index_col=0)\n",
    "\n",
    "test_record_df = pd.read_csv('./data/test_record_df.csv', index_col=0)\n",
    "test_attrib_df = pd.read_csv('./data/test_attrib_df.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4990</th>\n",
       "      <th>4991</th>\n",
       "      <th>4992</th>\n",
       "      <th>4993</th>\n",
       "      <th>4994</th>\n",
       "      <th>4995</th>\n",
       "      <th>4996</th>\n",
       "      <th>4997</th>\n",
       "      <th>4998</th>\n",
       "      <th>4999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.015</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.037</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.077</td>\n",
       "      <td>0.106</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "      <td>-0.150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.025</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "      <td>-0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.039</td>\n",
       "      <td>0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14773</th>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.107</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>-0.102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.039</td>\n",
       "      <td>-0.043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14774</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.017</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14775</th>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.034</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>-0.038</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "      <td>-0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14776</th>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14777</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>-0.015</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14778 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0      1      2      3      4      5      6      7      8      9  \\\n",
       "0      0.015  0.015  0.015  0.014  0.017  0.015 -0.023 -0.024  0.019  0.037   \n",
       "1      0.025  0.025  0.025  0.024  0.026  0.025  0.005  0.030  0.077  0.106   \n",
       "2     -0.025 -0.025 -0.025 -0.025 -0.025 -0.025 -0.029 -0.029 -0.033 -0.042   \n",
       "3      0.025  0.025  0.025  0.025  0.025  0.025  0.025  0.025  0.025  0.025   \n",
       "4     -0.039 -0.039 -0.039 -0.039 -0.039 -0.039 -0.039 -0.039 -0.043 -0.048   \n",
       "...      ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "14773 -0.107 -0.107 -0.107 -0.107 -0.107 -0.107 -0.107 -0.107 -0.102 -0.102   \n",
       "14774  0.015  0.015  0.015  0.015  0.015  0.015  0.015  0.015  0.014  0.017   \n",
       "14775 -0.040 -0.040 -0.040 -0.040 -0.040 -0.040 -0.036 -0.034 -0.038 -0.038   \n",
       "14776  0.015  0.015  0.015  0.015  0.015  0.015  0.023  0.026  0.026  0.032   \n",
       "14777 -0.010 -0.010 -0.010 -0.010 -0.010 -0.010 -0.010 -0.013 -0.015 -0.016   \n",
       "\n",
       "       ...   4990   4991   4992   4993   4994   4995   4996   4997   4998  \\\n",
       "0      ... -0.030 -0.030 -0.030 -0.030 -0.030 -0.030 -0.030 -0.030 -0.030   \n",
       "1      ... -0.150 -0.150 -0.150 -0.150 -0.150 -0.150 -0.150 -0.150 -0.150   \n",
       "2      ... -0.035 -0.035 -0.035 -0.035 -0.035 -0.035 -0.035 -0.035 -0.035   \n",
       "3      ... -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095 -0.095   \n",
       "4      ...  0.019  0.024  0.024  0.024  0.029  0.034  0.039  0.039  0.039   \n",
       "...    ...    ...    ...    ...    ...    ...    ...    ...    ...    ...   \n",
       "14773  ... -0.029 -0.029 -0.029 -0.034 -0.034 -0.034 -0.034 -0.039 -0.039   \n",
       "14774  ... -0.130 -0.130 -0.130 -0.130 -0.130 -0.130 -0.130 -0.130 -0.130   \n",
       "14775  ... -0.055 -0.055 -0.055 -0.055 -0.055 -0.055 -0.055 -0.055 -0.055   \n",
       "14776  ...  0.355  0.355  0.355  0.355  0.355  0.355  0.355  0.355  0.355   \n",
       "14777  ...  0.195  0.195  0.195  0.195  0.195  0.195  0.195  0.195  0.195   \n",
       "\n",
       "        4999  \n",
       "0     -0.030  \n",
       "1     -0.150  \n",
       "2     -0.035  \n",
       "3     -0.095  \n",
       "4      0.043  \n",
       "...      ...  \n",
       "14773 -0.043  \n",
       "14774 -0.130  \n",
       "14775 -0.055  \n",
       "14776  0.355  \n",
       "14777  0.195  \n",
       "\n",
       "[14778 rows x 5000 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_record_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>59.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 3 8</td>\n",
       "      <td>76.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>28.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>54.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>60.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19207</th>\n",
       "      <td>1 3</td>\n",
       "      <td>69.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19208</th>\n",
       "      <td>7</td>\n",
       "      <td>69.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19209</th>\n",
       "      <td>4</td>\n",
       "      <td>66.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19210</th>\n",
       "      <td>8</td>\n",
       "      <td>66.0</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19211</th>\n",
       "      <td>3</td>\n",
       "      <td>74.0</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label   age sex\n",
       "0          8  59.0   F\n",
       "1      2 3 8  76.0   F\n",
       "2          8  28.0   M\n",
       "3         11  54.0   F\n",
       "4         10  60.0   M\n",
       "...      ...   ...  ..\n",
       "19207    1 3  69.0   M\n",
       "19208      7  69.0   M\n",
       "19209      4  66.0   M\n",
       "19210      8  66.0   F\n",
       "19211      3  74.0   M\n",
       "\n",
       "[19212 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attrib_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_attrib_df['age'] = scaler.fit_transform(np.array(train_attrib_df['age']).reshape(-1,1)).flatten()\n",
    "train_attrib_df['age'] = train_attrib_df['age'].fillna(0.0)\n",
    "\n",
    "test_attrib_df['age'] = scaler.transform(np.array(test_attrib_df['age']).reshape(-1,1)).flatten()\n",
    "test_attrib_df['age'] = test_attrib_df['age'].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>-0.056717</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2 3 8</td>\n",
       "      <td>0.964791</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>-1.919467</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>-0.357161</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19207</th>\n",
       "      <td>1 3</td>\n",
       "      <td>0.544170</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19208</th>\n",
       "      <td>7</td>\n",
       "      <td>0.544170</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19209</th>\n",
       "      <td>4</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19210</th>\n",
       "      <td>8</td>\n",
       "      <td>0.363904</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19211</th>\n",
       "      <td>3</td>\n",
       "      <td>0.844613</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19212 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label       age sex\n",
       "0          8 -0.056717   F\n",
       "1      2 3 8  0.964791   F\n",
       "2          8 -1.919467   M\n",
       "3         11 -0.357161   F\n",
       "4         10  0.003371   M\n",
       "...      ...       ...  ..\n",
       "19207    1 3  0.544170   M\n",
       "19208      7  0.544170   M\n",
       "19209      4  0.363904   M\n",
       "19210      8  0.363904   F\n",
       "19211      3  0.844613   M\n",
       "\n",
       "[19212 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_attrib_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018298,
     "end_time": "2021-05-14T11:09:14.681352",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.663054",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:09:14.737220Z",
     "iopub.status.busy": "2021-05-14T11:09:14.736145Z",
     "iopub.status.idle": "2021-05-14T11:09:14.739262Z",
     "shell.execute_reply": "2021-05-14T11:09:14.738758Z"
    },
    "papermill": {
     "duration": 0.039374,
     "end_time": "2021-05-14T11:09:14.739407",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.700033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_files(is_training=True):\n",
    "    list_id = []\n",
    "    list_age = []\n",
    "    list_sex = []\n",
    "    list_recording = []\n",
    "    list_labels = []\n",
    "    \n",
    "    target_record_df = train_record_df if is_training else test_record_df\n",
    "    target_attrib_df = train_attrib_df if is_training else test_attrib_df\n",
    "\n",
    "    n = len(target_attrib_df)\n",
    "    for index in range(n):\n",
    "        list_id.append(index)\n",
    "        list_age.append(float(target_attrib_df.iloc[index]['age']))\n",
    "        list_sex.append(target_attrib_df.iloc[index]['sex'])\n",
    "        list_recording.append(np.array(target_record_df.iloc[[2*index, 2*index+1]]))\n",
    "\n",
    "    if is_training:\n",
    "        for index in range(n):\n",
    "            list_labels.append(target_attrib_df.iloc[index]['label'].split())\n",
    "        return list(zip(list_id, list_age, list_sex, list_recording, list_labels))\n",
    "\n",
    "    else:\n",
    "        return list(zip(list_id, list_age, list_sex, list_recording))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018248,
     "end_time": "2021-05-14T11:09:14.776399",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.758151",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PyTorch Custom Dataset\n",
    "training sample을 batch 단위로 처리할 수 있도록 torch.uitls.data.Dataset을 이용한 custom dataset을 만들어 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:09:14.825991Z",
     "iopub.status.busy": "2021-05-14T11:09:14.825210Z",
     "iopub.status.idle": "2021-05-14T11:09:14.828411Z",
     "shell.execute_reply": "2021-05-14T11:09:14.827925Z"
    },
    "papermill": {
     "duration": 0.033677,
     "end_time": "2021-05-14T11:09:14.828542",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.794865",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset_ECG(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        Build ECG dataset\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset, num_classes=12):\n",
    "        \"\"\"\n",
    "            dataset을 읽어들여 id, age, sex, recording, labels를 저장한 list를 만들어 줍니다.\n",
    "        \"\"\"\n",
    "        self.sample_id = []\n",
    "        self.sample_age = []\n",
    "        self.sample_sex = []\n",
    "        self.sample_recording = []\n",
    "        self.sample_labels = []\n",
    "        self.num_samples = len(dataset)\n",
    "        \n",
    "        for idx in range(self.num_samples):\n",
    "            _id, _age, _sex, _recording, _labels = dataset[idx]\n",
    "            # model에 input으로 들어가는 data는 torch.Tensor 타입으로 변환해 줍니다.\n",
    "            age = torch.tensor(_age)\n",
    "            sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n",
    "            recording = torch.tensor(_recording)\n",
    "            labels = torch.tensor(np.zeros(num_classes))\n",
    "            for label in _labels:\n",
    "                labels[int(label)] = 1\n",
    "\n",
    "            self.sample_id.append(_id)\n",
    "            self.sample_age.append(age)\n",
    "            self.sample_sex.append(sex)\n",
    "            self.sample_recording.append(recording)\n",
    "            self.sample_labels.append(labels)\n",
    "\n",
    "        print(f'Loaded {self.num_samples} samples...')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"id\": self.sample_id[idx],\n",
    "            \"age\": self.sample_age[idx],\n",
    "            \"sex\": self.sample_sex[idx],\n",
    "            \"recording\": self.sample_recording[idx],\n",
    "            \"labels\": self.sample_labels[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017992,
     "end_time": "2021-05-14T11:09:14.866131",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.848139",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PyTorch CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Sequential, ReLU, Conv1d, Linear, BatchNorm1d, Dropout, MaxPool1d\n",
    "\n",
    "class Feedback_CNN(Module):\n",
    "    def __init__(self, num_classes=12, num_leads=2):\n",
    "        super(Feedback_CNN, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_leads = num_leads\n",
    "        \n",
    "        #self.conv = Conv1d(in_channels=self.num_leads, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #self.batch_norm = BatchNorm1d(32)\n",
    "        \n",
    "        self.relu = ReLU()\n",
    "\n",
    "        self.conv1 = Conv1d(in_channels=self.num_leads, out_channels=32, kernel_size=15, stride=3, padding=2)\n",
    "        self.conv_pad1 = Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool1 = MaxPool1d(kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_norm1 = BatchNorm1d(32)\n",
    "        \n",
    "        self.conv2 = Conv1d(in_channels=32, out_channels=64, kernel_size=13, stride=3, padding=1)\n",
    "        self.conv_pad2 = Conv1d(in_channels=64, out_channels=64, kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool2 = MaxPool1d(kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_norm2= BatchNorm1d(64)\n",
    "\n",
    "        self.conv3 = Conv1d(in_channels=64, out_channels=128, kernel_size=10, stride=2)\n",
    "        self.conv_pad3 = Conv1d(in_channels=128, out_channels=128, kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool3 = MaxPool1d(kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_norm3 = BatchNorm1d(128)\n",
    "\n",
    "        self.conv4 = Conv1d(in_channels=128, out_channels=64, kernel_size=7, stride=2)\n",
    "        self.conv_pad4 = Conv1d(in_channels=64, out_channels=64, kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool4 = MaxPool1d(kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_norm4 = BatchNorm1d(64)\n",
    "\n",
    "        self.conv5 = Conv1d(in_channels=64, out_channels=32, kernel_size=6, stride=2)\n",
    "        self.conv_pad5 = Conv1d(in_channels=32, out_channels=32, kernel_size=5,stride=1,padding=2)\n",
    "        self.max_pool5 = MaxPool1d(kernel_size=5, stride=1, padding=2)\n",
    "        self.batch_norm5 = BatchNorm1d(32)\n",
    "\n",
    "        self.drop_out = Dropout(p = 0.3)\n",
    "\n",
    "        self.fc1 = Linear(32*64, 128)\n",
    "        self.fc2 = Linear(130, self.num_classes)\n",
    "\n",
    "        # for age, sex\n",
    "        self.ln1 = Linear(2,32)\n",
    "        self.ln2 = Linear(32,2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # 이 모델은 recording만을 input으로 받습니다. feature를 추가적으로 사용하도록 할 수도 있습니다.\n",
    "        x = input[0]\n",
    "        x_age = input[1]\n",
    "        x_sex = input[2]\n",
    "        \n",
    "        #print(x.size()) # torch.Size([64, 2, 5000])\n",
    "        x = self.conv1(x)\n",
    "        x1 = x\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_pad1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = x + x1\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.drop_out(x)\n",
    "        \n",
    "        #print(x.size()) # torch.Size([64, 32, 1664])\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x2 = x\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_pad2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = x + x2\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        #print(x.size()) # torch.Size([64, 64, 552])\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x3 = x\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_pad3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = x + x3\n",
    "        x = self.relu(x)\n",
    "        x = self.drop_out(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        #print(x.size()) # torch.Size([64, 128, 272])\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x4 = x\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_pad4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = x + x4\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool4(x)\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        #print(x.size()) # torch.Size([64, 64, 133])\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x5 = x\n",
    "        x = self.relu(x)\n",
    "        x = self.conv_pad5(x)\n",
    "        x = self.batch_norm5(x)\n",
    "        x = x + x5\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool5(x)\n",
    "        x = self.drop_out(x)\n",
    "\n",
    "        #print(x.size()) # torch.Size([64, 32, 64])\n",
    "        \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        #print(x.size()) # torch.Size([64, 2048])\n",
    "        x = self.fc1(x)\n",
    "        #print(x.size())\n",
    "        x = self.relu(x) # torch.Size([64, 128])\n",
    "\n",
    "        x_age_sex = self.ln1(torch.cat((x_age,x_sex), dim = 1))\n",
    "        x_age_sex = self.relu(x_age_sex)\n",
    "        x_age_sex = self.ln2(x_age_sex)\n",
    "\n",
    "        out = self.fc2(torch.cat((x,x_age_sex),dim = 1))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018337,
     "end_time": "2021-05-14T11:09:14.956898",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.938561",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:09:15.000213Z",
     "iopub.status.busy": "2021-05-14T11:09:14.999420Z",
     "iopub.status.idle": "2021-05-14T11:13:57.968320Z",
     "shell.execute_reply": "2021-05-14T11:13:57.969071Z"
    },
    "papermill": {
     "duration": 282.993852,
     "end_time": "2021-05-14T11:13:57.969308",
     "exception": false,
     "start_time": "2021-05-14T11:09:14.975456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total training samples: 19212\n",
      "Number of validation samples: 0\n",
      "Number of training samples: 19212\n"
     ]
    }
   ],
   "source": [
    "total_training_set = sorted(read_files(training_dir), key=lambda sample: sample[0])\n",
    "total_num_training = len(total_training_set)\n",
    "print(f\"Number of total training samples: {total_num_training}\")\n",
    "\n",
    "num_validation = int(total_num_training * 0.0)\n",
    "num_training = total_num_training - num_validation\n",
    "\n",
    "validation_set = total_training_set[:num_validation]\n",
    "training_set = total_training_set[num_validation:]\n",
    "\n",
    "print(f'Number of validation samples: {num_validation}')\n",
    "print(f'Number of training samples: {num_training}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:58.013526Z",
     "iopub.status.busy": "2021-05-14T11:13:58.012808Z",
     "iopub.status.idle": "2021-05-14T11:13:58.016036Z",
     "shell.execute_reply": "2021-05-14T11:13:58.016610Z"
    },
    "papermill": {
     "duration": 0.027534,
     "end_time": "2021-05-14T11:13:58.016818",
     "exception": false,
     "start_time": "2021-05-14T11:13:57.989284",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# cuda gpu를 사용할 수 있을 경우 사용합니다.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:58.066120Z",
     "iopub.status.busy": "2021-05-14T11:13:58.065432Z",
     "iopub.status.idle": "2021-05-14T11:13:59.350438Z",
     "shell.execute_reply": "2021-05-14T11:13:59.350899Z"
    },
    "papermill": {
     "duration": 1.314873,
     "end_time": "2021-05-14T11:13:59.351108",
     "exception": false,
     "start_time": "2021-05-14T11:13:58.036235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 19212 samples...\n"
     ]
    }
   ],
   "source": [
    "# 위에서 정의한 Dataset_ECG를 활용해 training dataset을 만들어 줍니다.\n",
    "training_dataset = Dataset_ECG(training_set, num_classes=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:59.397195Z",
     "iopub.status.busy": "2021-05-14T11:13:59.396186Z",
     "iopub.status.idle": "2021-05-14T11:13:59.400441Z",
     "shell.execute_reply": "2021-05-14T11:13:59.399460Z"
    },
    "papermill": {
     "duration": 0.029476,
     "end_time": "2021-05-14T11:13:59.400630",
     "exception": false,
     "start_time": "2021-05-14T11:13:59.371154",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training에 사용될 hyperparameter를 정해줍니다.\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:59.446509Z",
     "iopub.status.busy": "2021-05-14T11:13:59.445851Z",
     "iopub.status.idle": "2021-05-14T11:13:59.449248Z",
     "shell.execute_reply": "2021-05-14T11:13:59.448702Z"
    },
    "papermill": {
     "duration": 0.0285,
     "end_time": "2021-05-14T11:13:59.449380",
     "exception": false,
     "start_time": "2021-05-14T11:13:59.420880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training dataset을 batch 단위로 읽어들일 수 있도록 DataLoader를 만들어줍니다.\n",
    "training_loader = torch.utils.data.DataLoader(training_dataset, pin_memory=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020035,
     "end_time": "2021-05-14T11:13:59.489647",
     "exception": false,
     "start_time": "2021-05-14T11:13:59.469612",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:59.544525Z",
     "iopub.status.busy": "2021-05-14T11:13:59.543848Z",
     "iopub.status.idle": "2021-05-14T11:13:59.563912Z",
     "shell.execute_reply": "2021-05-14T11:13:59.563341Z"
    },
    "papermill": {
     "duration": 0.054234,
     "end_time": "2021-05-14T11:13:59.564083",
     "exception": false,
     "start_time": "2021-05-14T11:13:59.509849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model = myCNN(num_classes=12, num_leads=2)\n",
    "#model = Example_CNN_v1(num_classes=12, num_leads=2)\n",
    "#model = Anomaly_Classifier(num_classes=12, num_leads=2)\n",
    "#model = CNN_v1(num_classes=12, num_leads=2)\n",
    "model = Feedback_CNN(num_classes=12, num_leads=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss() # for multi-label classification\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:13:59.615305Z",
     "iopub.status.busy": "2021-05-14T11:13:59.614309Z",
     "iopub.status.idle": "2021-05-14T11:36:19.464848Z",
     "shell.execute_reply": "2021-05-14T11:36:19.465620Z"
    },
    "papermill": {
     "duration": 1339.881626,
     "end_time": "2021-05-14T11:36:19.466193",
     "exception": false,
     "start_time": "2021-05-14T11:13:59.584567",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Epoch 1 *****\n",
      "training loss of epoch 1: 0.22656420628381513\n",
      "\n",
      "***** Epoch 2 *****\n",
      "training loss of epoch 2: 0.18109528363084243\n",
      "\n",
      "***** Epoch 3 *****\n",
      "training loss of epoch 3: 0.16708303606279015\n",
      "\n",
      "***** Epoch 4 *****\n",
      "training loss of epoch 4: 0.15786691700735497\n",
      "\n",
      "***** Epoch 5 *****\n",
      "training loss of epoch 5: 0.1517730571424035\n",
      "\n",
      "***** Epoch 6 *****\n",
      "training loss of epoch 6: 0.14752754514141514\n",
      "\n",
      "***** Epoch 7 *****\n",
      "training loss of epoch 7: 0.14415646218742656\n",
      "\n",
      "***** Epoch 8 *****\n",
      "training loss of epoch 8: 0.14153561553954785\n",
      "\n",
      "***** Epoch 9 *****\n",
      "training loss of epoch 9: 0.1389106779011764\n",
      "\n",
      "***** Epoch 10 *****\n",
      "training loss of epoch 10: 0.135737525972891\n",
      "\n",
      "***** Epoch 11 *****\n",
      "training loss of epoch 11: 0.1341944524774046\n",
      "\n",
      "***** Epoch 12 *****\n",
      "training loss of epoch 12: 0.13160689659778563\n",
      "\n",
      "***** Epoch 13 *****\n",
      "training loss of epoch 13: 0.13004380273179206\n",
      "\n",
      "***** Epoch 14 *****\n",
      "training loss of epoch 14: 0.1284872445842559\n",
      "\n",
      "***** Epoch 15 *****\n",
      "training loss of epoch 15: 0.127571176860968\n",
      "\n",
      "***** Epoch 16 *****\n",
      "training loss of epoch 16: 0.1262689650834453\n",
      "\n",
      "***** Epoch 17 *****\n",
      "training loss of epoch 17: 0.1241656420780058\n",
      "\n",
      "***** Epoch 18 *****\n",
      "training loss of epoch 18: 0.12246848817744384\n",
      "\n",
      "***** Epoch 19 *****\n",
      "training loss of epoch 19: 0.12227342397816417\n",
      "\n",
      "***** Epoch 20 *****\n",
      "training loss of epoch 20: 0.12047621712359351\n",
      "\n",
      "***** Epoch 21 *****\n",
      "training loss of epoch 21: 0.11995917690318406\n",
      "\n",
      "***** Epoch 22 *****\n",
      "training loss of epoch 22: 0.11851395128976192\n",
      "\n",
      "***** Epoch 23 *****\n",
      "training loss of epoch 23: 0.11783482550571635\n",
      "\n",
      "***** Epoch 24 *****\n",
      "training loss of epoch 24: 0.11780497249953496\n",
      "\n",
      "***** Epoch 25 *****\n",
      "training loss of epoch 25: 0.10866261969065277\n",
      "\n",
      "***** Epoch 26 *****\n",
      "training loss of epoch 26: 0.10529200325642415\n",
      "\n",
      "***** Epoch 27 *****\n",
      "training loss of epoch 27: 0.10464608073985618\n",
      "\n",
      "***** Epoch 28 *****\n",
      "training loss of epoch 28: 0.10281461658944764\n",
      "\n",
      "***** Epoch 29 *****\n",
      "training loss of epoch 29: 0.10213577858200588\n",
      "\n",
      "***** Epoch 30 *****\n",
      "training loss of epoch 30: 0.1013692315526057\n",
      "\n",
      "***** Epoch 31 *****\n",
      "training loss of epoch 31: 0.10061161289000446\n",
      "\n",
      "***** Epoch 32 *****\n",
      "training loss of epoch 32: 0.09999032383094177\n",
      "\n",
      "***** Epoch 33 *****\n",
      "training loss of epoch 33: 0.09958489161416696\n",
      "\n",
      "***** Epoch 34 *****\n",
      "training loss of epoch 34: 0.09871020847662787\n",
      "\n",
      "***** Epoch 35 *****\n",
      "training loss of epoch 35: 0.09854132988131929\n",
      "\n",
      "***** Epoch 36 *****\n",
      "training loss of epoch 36: 0.09819352503384153\n",
      "\n",
      "***** Epoch 37 *****\n",
      "training loss of epoch 37: 0.09760293459926005\n",
      "\n",
      "***** Epoch 38 *****\n",
      "training loss of epoch 38: 0.09778633652043563\n",
      "\n",
      "***** Epoch 39 *****\n",
      "training loss of epoch 39: 0.0967379464882722\n",
      "\n",
      "***** Epoch 40 *****\n",
      "training loss of epoch 40: 0.09643920021710568\n",
      "\n",
      "***** Epoch 41 *****\n",
      "training loss of epoch 41: 0.09644261419552433\n",
      "\n",
      "***** Epoch 42 *****\n",
      "training loss of epoch 42: 0.0962040295262552\n",
      "\n",
      "***** Epoch 43 *****\n",
      "training loss of epoch 43: 0.09512889562613837\n",
      "\n",
      "***** Epoch 44 *****\n",
      "training loss of epoch 44: 0.09541122217876663\n",
      "\n",
      "***** Epoch 45 *****\n",
      "training loss of epoch 45: 0.09442637534393535\n",
      "\n",
      "***** Epoch 46 *****\n",
      "training loss of epoch 46: 0.09419541662921005\n",
      "\n",
      "***** Epoch 47 *****\n",
      "training loss of epoch 47: 0.09337412931270303\n",
      "\n",
      "***** Epoch 48 *****\n",
      "training loss of epoch 48: 0.09375401467291539\n",
      "\n",
      "***** Epoch 49 *****\n",
      "training loss of epoch 49: 0.09343022362338435\n",
      "\n",
      "***** Epoch 50 *****\n",
      "training loss of epoch 50: 0.0933788970346671\n",
      "\n",
      "***** Epoch 51 *****\n",
      "training loss of epoch 51: 0.09244746223610513\n",
      "\n",
      "***** Epoch 52 *****\n",
      "training loss of epoch 52: 0.09193793757439572\n",
      "\n",
      "***** Epoch 53 *****\n",
      "training loss of epoch 53: 0.09166907013130587\n",
      "\n",
      "***** Epoch 54 *****\n",
      "training loss of epoch 54: 0.09135720852954332\n",
      "\n",
      "***** Epoch 55 *****\n",
      "training loss of epoch 55: 0.09103488694302561\n",
      "\n",
      "***** Epoch 56 *****\n",
      "training loss of epoch 56: 0.09052959084359045\n",
      "\n",
      "***** Epoch 57 *****\n",
      "training loss of epoch 57: 0.09070720012087037\n",
      "\n",
      "***** Epoch 58 *****\n",
      "training loss of epoch 58: 0.0903865503600936\n",
      "\n",
      "***** Epoch 59 *****\n",
      "training loss of epoch 59: 0.09012093273699426\n",
      "\n",
      "***** Epoch 60 *****\n",
      "training loss of epoch 60: 0.08850697849990674\n",
      "\n",
      "***** Epoch 61 *****\n",
      "training loss of epoch 61: 0.0883732624855338\n",
      "\n",
      "***** Epoch 62 *****\n",
      "training loss of epoch 62: 0.08837002260659971\n",
      "\n",
      "***** Epoch 63 *****\n",
      "training loss of epoch 63: 0.08810580730076482\n",
      "\n",
      "***** Epoch 64 *****\n",
      "training loss of epoch 64: 0.08805114262933177\n",
      "\n",
      "***** Epoch 65 *****\n",
      "training loss of epoch 65: 0.08774526117634703\n",
      "\n",
      "***** Epoch 66 *****\n",
      "training loss of epoch 66: 0.0878761264893204\n",
      "\n",
      "***** Epoch 67 *****\n",
      "training loss of epoch 67: 0.08774764041392241\n",
      "\n",
      "***** Epoch 68 *****\n",
      "training loss of epoch 68: 0.08719542967937559\n",
      "\n",
      "***** Epoch 69 *****\n",
      "training loss of epoch 69: 0.08701881407102056\n",
      "\n",
      "***** Epoch 70 *****\n",
      "training loss of epoch 70: 0.0872077404622084\n",
      "\n",
      "***** Epoch 71 *****\n",
      "training loss of epoch 71: 0.08724215553997945\n",
      "\n",
      "***** Epoch 72 *****\n",
      "training loss of epoch 72: 0.08776036389911847\n",
      "\n",
      "***** Epoch 73 *****\n",
      "training loss of epoch 73: 0.08740967022731463\n",
      "\n",
      "***** Epoch 74 *****\n",
      "training loss of epoch 74: 0.08758579473040229\n",
      "\n",
      "***** Epoch 75 *****\n",
      "training loss of epoch 75: 0.08707018779388441\n",
      "\n",
      "***** Epoch 76 *****\n",
      "training loss of epoch 76: 0.08718809742502621\n",
      "\n",
      "***** Epoch 77 *****\n",
      "training loss of epoch 77: 0.0866036830791171\n",
      "\n",
      "***** Epoch 78 *****\n",
      "training loss of epoch 78: 0.08677843242222202\n",
      "\n",
      "***** Epoch 79 *****\n",
      "training loss of epoch 79: 0.08742193111350599\n",
      "\n",
      "***** Epoch 80 *****\n",
      "training loss of epoch 80: 0.08730011462866291\n",
      "\n",
      "***** Epoch 81 *****\n",
      "training loss of epoch 81: 0.08702461555202372\n",
      "\n",
      "***** Epoch 82 *****\n",
      "training loss of epoch 82: 0.08742133579463626\n",
      "\n",
      "***** Epoch 83 *****\n",
      "training loss of epoch 83: 0.08684209629684124\n",
      "\n",
      "***** Epoch 84 *****\n",
      "training loss of epoch 84: 0.08726812798102349\n",
      "\n",
      "***** Epoch 85 *****\n",
      "training loss of epoch 85: 0.08656466669228752\n",
      "\n",
      "***** Epoch 86 *****\n",
      "training loss of epoch 86: 0.08655237933579603\n",
      "\n",
      "***** Epoch 87 *****\n",
      "training loss of epoch 87: 0.08662804141661203\n",
      "\n",
      "***** Epoch 88 *****\n",
      "training loss of epoch 88: 0.08658581619534698\n",
      "\n",
      "***** Epoch 89 *****\n",
      "training loss of epoch 89: 0.08711774558844036\n",
      "\n",
      "***** Epoch 90 *****\n",
      "training loss of epoch 90: 0.08691529309600708\n",
      "\n",
      "***** Epoch 91 *****\n",
      "training loss of epoch 91: 0.08654963364610192\n",
      "\n",
      "***** Epoch 92 *****\n",
      "training loss of epoch 92: 0.08695003414694961\n",
      "\n",
      "***** Epoch 93 *****\n",
      "training loss of epoch 93: 0.08659787416141514\n",
      "\n",
      "***** Epoch 94 *****\n",
      "training loss of epoch 94: 0.08712315347304847\n",
      "\n",
      "***** Epoch 95 *****\n",
      "training loss of epoch 95: 0.08690953638820144\n",
      "\n",
      "***** Epoch 96 *****\n",
      "training loss of epoch 96: 0.08711543510387072\n",
      "\n",
      "***** Epoch 97 *****\n",
      "training loss of epoch 97: 0.08652933485359877\n",
      "\n",
      "***** Epoch 98 *****\n",
      "training loss of epoch 98: 0.086583383170281\n",
      "\n",
      "***** Epoch 99 *****\n",
      "training loss of epoch 99: 0.08581462706449455\n",
      "\n",
      "***** Epoch 100 *****\n",
      "training loss of epoch 100: 0.08602901631893854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'***** Epoch {epoch} *****')\n",
    "    epoch_training_loss_sum = 0.0\n",
    "    if epoch == 25:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-4\n",
    "    if epoch == 60:\n",
    "        optimizer.param_groups[0]['lr'] = 1e-5\n",
    "    for i_batch, sample_batched in enumerate(training_loader):\n",
    "        b_recording = sample_batched[\"recording\"].to(device)\n",
    "        b_age = sample_batched[\"age\"].to(device)\n",
    "        b_sex = sample_batched[\"sex\"].to(device)\n",
    "        b_in = [b_recording.float(), torch.reshape(b_age,(-1,1)), torch.reshape(b_sex,(-1,1))]\n",
    "        b_labels = sample_batched[\"labels\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        b_out = model(b_in)\n",
    "        loss = criterion(b_out, b_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_training_loss_sum += loss.item() * b_labels.shape[0]\n",
    "\n",
    "    epoch_training_loss = epoch_training_loss_sum / num_training\n",
    "    print(f'training loss of epoch {epoch}: {epoch_training_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026568,
     "end_time": "2021-05-14T11:36:19.520776",
     "exception": false,
     "start_time": "2021-05-14T11:36:19.494208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation\n",
    "evalutate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "validation_prediction_df = pd.DataFrame(columns=['labels'])\n",
    "validation_prediction_df.index.name = 'id'\n",
    "validation_true_labels_df = pd.DataFrame(columns=['labels'])\n",
    "validation_true_labels_df.index.name = 'id'\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(validation_set)):\n",
    "        validation_sample = validation_set[idx]\n",
    "        _, _age, _sex, _recording, labels = validation_sample\n",
    "        age = torch.tensor(_age)\n",
    "        sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n",
    "        recording = torch.tensor(_recording, dtype = torch.float32)\n",
    "\n",
    "        valid_in = [recording.unsqueeze(0).to(device), \n",
    "                   torch.reshape(age,(-1,1)).to(device), \n",
    "                   torch.reshape(sex,(-1,1)).to(device)]\n",
    "  \n",
    "        out = model(valid_in) \n",
    "        sample_prediction = np.array(torch.sigmoid(out).squeeze().cpu())\n",
    "        pred_list.append(sample_prediction)\n",
    "        \n",
    "        str_true_labels = ' '.join(labels)\n",
    "        validation_true_labels_df.loc[idx] = [str_true_labels]\n",
    "  \n",
    "th_arr = np.array([0.25, 0.25, 0.25, 0.35, 0.15, 0.15, 0.25, 0.5, 0.4, 0.55, 0.25, 0.05])\n",
    "\n",
    "valid_pred = np.array(pred_list) > th_arr\n",
    "\n",
    "for idx in range(len(valid_pred)):\n",
    "    valid_prediction = valid_pred[idx]\n",
    "    indices_of_1s = np.where(valid_prediction)[0]\n",
    "    str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n",
    "    validation_prediction_df.loc[idx] = [str_indices_of_1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:36:19.584195Z",
     "iopub.status.busy": "2021-05-14T11:36:19.583491Z",
     "iopub.status.idle": "2021-05-14T11:36:48.926053Z",
     "shell.execute_reply": "2021-05-14T11:36:48.925466Z"
    },
    "papermill": {
     "duration": 29.378671,
     "end_time": "2021-05-14T11:36:48.926212",
     "exception": false,
     "start_time": "2021-05-14T11:36:19.547541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# validation_prediction_df = pd.DataFrame(columns=['labels'])\n",
    "# validation_prediction_df.index.name = 'id'\n",
    "# validation_true_labels_df = pd.DataFrame(columns=['labels'])\n",
    "# validation_true_labels_df.index.name = 'id'\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for idx in range(len(validation_set)):\n",
    "#         validation_sample = validation_set[idx]\n",
    "#         _, _age, _sex, _recording, labels = validation_sample\n",
    "        \n",
    "#         age = torch.tensor(_age)\n",
    "#         sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n",
    "#         recording = torch.tensor(_recording, dtype = torch.float32)\n",
    "        \n",
    "#         valid_in = [recording.unsqueeze(0).to(device), \n",
    "#                    torch.reshape(age,(-1,1)).to(device), \n",
    "#                    torch.reshape(sex,(-1,1)).to(device)]\n",
    "        \n",
    "#         out = model(valid_in) # unsqueeze는 batch dimension을 추가해주기 위함\n",
    "#         sample_prediction = torch.sigmoid(out).squeeze() > 0.15 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n",
    "#         indices_of_1s = np.where(sample_prediction.cpu())[0]\n",
    "#         str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n",
    "#         validation_prediction_df.loc[idx] = [str_indices_of_1s]\n",
    "        \n",
    "#         str_true_labels = ' '.join(labels)\n",
    "#         validation_true_labels_df.loc[idx] = [str_true_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:36:48.988024Z",
     "iopub.status.busy": "2021-05-14T11:36:48.987356Z",
     "iopub.status.idle": "2021-05-14T11:36:48.997122Z",
     "shell.execute_reply": "2021-05-14T11:36:48.996588Z"
    },
    "papermill": {
     "duration": 0.043191,
     "end_time": "2021-05-14T11:36:48.997257",
     "exception": false,
     "start_time": "2021-05-14T11:36:48.954066",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "id       \n",
      "0       8\n",
      "1   2 3 8\n",
      "2       8\n",
      "3      10\n",
      "4       8\n",
      "5     3 9\n",
      "6       8\n",
      "7       8\n",
      "8       9\n",
      "9       8\n"
     ]
    }
   ],
   "source": [
    "print(validation_prediction_df[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:36:49.059764Z",
     "iopub.status.busy": "2021-05-14T11:36:49.059155Z",
     "iopub.status.idle": "2021-05-14T11:36:49.062331Z",
     "shell.execute_reply": "2021-05-14T11:36:49.063058Z"
    },
    "papermill": {
     "duration": 0.037905,
     "end_time": "2021-05-14T11:36:49.063276",
     "exception": false,
     "start_time": "2021-05-14T11:36:49.025371",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   labels\n",
      "id       \n",
      "0       8\n",
      "1   2 3 8\n",
      "2       8\n",
      "3      11\n",
      "4      10\n",
      "5     3 8\n",
      "6       8\n",
      "7      10\n",
      "8       8\n",
      "9       8\n"
     ]
    }
   ],
   "source": [
    "print(validation_true_labels_df[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation Set 결과 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, **0.55**, **0.25**, **0.1**])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8852459  0.53684922 0.2254902 ]\n",
      "macro f1 score on validation set: 0.6300749865814018\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, **0.55**, **0.25**, 0.15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8852459  0.53684922 0.22121896]\n",
      "macro f1 score on validation set: 0.6297190503769726\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, **0.55**, **0.25**, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8852459  0.53684922 0.13981763]\n",
      "macro f1 score on validation set: 0.6229356060064768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, **0.55**, **0.25**, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8852459  0.5257732  0.1027668 ]\n",
      "macro f1 score on validation set: 0.618925034562171\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, **0.55**, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8852459  0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6198480367764468\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.5, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.88447653 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6197839228311707\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.45, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.8765653  0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6191246529039971\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.4, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.86879433 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6184770721599294\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.35, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.86772487 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6183879506169072\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.3, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.86013986 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6177558666514898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, **0.4**, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94602327 0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.617671747871719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, 0.35, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94568933 0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6176439194252419\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, 0.3, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.94444444 0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.617540179148695\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, **0.5**, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87874016 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6174226758586029\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, 0.45, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.87363495 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6169972415183946\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, 0.4, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.86996904 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6166917494225498\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, 0.35, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.86809816 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6165358426943435\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, 0.3, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.86453577 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6162389767890231\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, **0.15**, **0.25**, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.21052632\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6160848766655915\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.18348624\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6138315368941446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기준! th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, **?0.15**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.43243243 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6116989117533108\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, 0.2, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.42567568 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6111358486902477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**기준!!** th_arr = np.array([**0.25**, **0.25**, **0.25**, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6093707408858141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, **0.25**, *0.2*, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.74796748 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6082918007824838\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([**0.25**, *0.2*, 0.25, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.85507246 0.76091476 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6090671465056342\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([*0.3*, 0.25, 0.25, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.66940452 0.8587156  0.76091476 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6088257561739621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([*0.2*, 0.25, 0.25, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.66539924 0.8587156  0.76091476 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6084919830147894\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([0.25, 0.25, 0.25, **0.35**, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.78304518 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6093707408858141\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([0.25, 0.25, 0.25, 0.3, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.77795963 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6089469453308346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "th_arr = np.array([0.25, 0.25, 0.25, 0.4, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.77739331 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6088997521009867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모두 0.25, 25, 60 learning rate 구간 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67594433 0.8587156  0.76091476 0.76816239 0.40449438 0.15789474\n",
      " 0.36697248 0.86268657 0.9430344  0.85913043 0.53684922 0.1027668 ]\n",
      "macro f1 score on validation set: 0.6081305090090792\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모두 0.25, weight_decay=1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.67836257 0.81911263 0.57754011 0.71316819 0.41317365 0.12408759\n",
      " 0.32846715 0.66524064 0.83211679 0.81089744 0.47296494 0.14854111]\n",
      "macro f1 score on validation set: 0.5486394013786351\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback_CNN with feedback threshold=0.25\n",
    "optimizer = Adam EPOCHS = 100 BATCH_SIZE = 64 LEARNING_RATE = 0.001(20부터 0.001/2, 40부터 0.0001)\n",
    "dropout=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.64435146 0.84476534 0.76673428 0.78255946 0.41767068 0.13333333\n",
      " 0.33663366 0.86676428 0.94222391 0.87867647 0.5266229  0.15972222]\n",
      "macro f1 score on validation set: 0.608338166686962\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.64347826 0.84460695 0.77366255 0.7817552  0.42622951 0.11827957\n",
      " 0.32167832 0.875      0.94164607 0.87732342 0.52777778 0.10569106]\n",
      "macro f1 score on validation set: 0.603094057084934\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.63691684 0.8348135  0.75049116 0.78135405 0.41198502 0.16393443\n",
      " 0.35185185 0.85632184 0.94343891 0.87410072 0.5253893  0.18181818]\n",
      "macro f1 score on validation set: 0.609367983233361\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.61895551 0.83216783 0.74339623 0.77408819 0.41522491 0.16271186\n",
      " 0.33810888 0.84151473 0.94161817 0.86678508 0.51915946 0.20674157]\n",
      "macro f1 score on validation set: 0.605039368834014\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------모델 구분선------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.651341   0.83754513 0.73684211 0.74595142 0.4379562  0.19130435\n",
      " 0.29538462 0.82885431 0.94205607 0.88256228 0.50590429 0.13043478]\n",
      "macro f1 score on validation set: 0.5988447124763057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback_CNN with feedback threshold=0.25\n",
    "optimizer = Adam\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001(20부터 1e-4, 40부터 1e-5)\n",
    "dropout=0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.6506986  0.83882784 0.74463938 0.75528078 0.43027888 0.1980198\n",
      " 0.28767123 0.84057971 0.94333959 0.88172043 0.52144772 0.10727969]\n",
      "macro f1 score on validation set: 0.5999819718684855\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.65135699 0.83858998 0.75449102 0.7606383  0.42677824 0.15469613\n",
      " 0.27338129 0.85376662 0.94427711 0.88648649 0.51589595 0.08571429]\n",
      "macro f1 score on validation set: 0.595506034423383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.35"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.65517241 0.84171322 0.76267748 0.76361656 0.4173913  0.12345679\n",
      " 0.27067669 0.86315789 0.94665153 0.89051095 0.51512801 0.07291667]\n",
      "macro f1 score on validation set: 0.5935891260817504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.66966292 0.84310019 0.77018634 0.7627024  0.40552995 0.09271523\n",
      " 0.232      0.86890244 0.94714829 0.89667897 0.50084317 0.04678363]\n",
      "macro f1 score on validation set: 0.5863544602687738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.66976744 0.84291188 0.77542373 0.7696726  0.38647343 0.04109589\n",
      " 0.21940928 0.87171561 0.94841421 0.89552239 0.48872858 0.03726708]\n",
      "macro f1 score on validation set: 0.5788668442883022\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.65491184 0.83921569 0.76853933 0.76621787 0.33684211 0.01470588\n",
      " 0.1981982  0.87244094 0.9458887  0.89943074 0.48571429 0.01333333]\n",
      "macro f1 score on validation set: 0.5662865756057421\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback_CNN with feedback(threshold=0.5)\n",
    "optimizer = Adam\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.66666667 0.84719536 0.77802198 0.76859504 0.35714286 0.02816901\n",
      " 0.20960699 0.87538941 0.94593558 0.89849624 0.48715509 0.02597403]\n",
      "macro f1 score on validation set: 0.5740290208215793\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback_CNN with feedback\n",
    "optimizer = Adam\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.65227818 0.85606061 0.75115207 0.76827757 0.37681159 0.04109589\n",
      " 0.1010101  0.85758998 0.94137931 0.9039548  0.47046843 0.02597403]\n",
      "macro f1 score on validation set: 0.5621710474023374\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedback_CNN with no feedback\n",
    "optimizer = Adam\n",
    "EPOCHS = 30\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.63938619 0.85090909 0.74172185 0.77118644 0.28235294 0.\n",
      " 0.17647059 0.81391304 0.93835098 0.88973384 0.35205993 0.        ]\n",
      "macro f1 score on validation set: 0.5380070747330329\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조교님 model 점수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:36:49.126066Z",
     "iopub.status.busy": "2021-05-14T11:36:49.125347Z",
     "iopub.status.idle": "2021-05-14T11:36:50.358879Z",
     "shell.execute_reply": "2021-05-14T11:36:50.359364Z"
    },
    "papermill": {
     "duration": 1.266008,
     "end_time": "2021-05-14T11:36:50.359545",
     "exception": false,
     "start_time": "2021-05-14T11:36:49.093537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macro f1 score on validation set: [0.45405405 0.67230444 0.53391685 0.64206642 0.04371585 0.06896552\n",
      " 0.03076923 0.73630137 0.9009716  0.84859813 0.32536765 0.08695652]\n",
      "macro f1 score on validation set: 0.44533230228243387\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=['0','1','2','3','4','5','6','7','8','9','10','11'])\n",
    "mlb.fit(map(str.split, validation_true_labels_df['labels'].values))\n",
    "\n",
    "macro_f1_validation = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average=None)\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation}')\n",
    "\n",
    "macro_f1_validation_whole = f1_score(mlb.transform(map(str.split, validation_true_labels_df['labels'].values)), mlb.transform(map(str.split, validation_prediction_df['labels'].values)), average='macro')\n",
    "print(f'macro f1 score on validation set: {macro_f1_validation_whole}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028538,
     "end_time": "2021-05-14T11:36:50.416631",
     "exception": false,
     "start_time": "2021-05-14T11:36:50.388093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Test Prediction\n",
    "학습된 모델로 test_set에 대한 prediction을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:36:50.479525Z",
     "iopub.status.busy": "2021-05-14T11:36:50.478763Z",
     "iopub.status.idle": "2021-05-14T11:38:38.229455Z",
     "shell.execute_reply": "2021-05-14T11:38:38.229980Z"
    },
    "papermill": {
     "duration": 107.784808,
     "end_time": "2021-05-14T11:38:38.230171",
     "exception": false,
     "start_time": "2021-05-14T11:36:50.445363",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of test samples: 7389\n"
     ]
    }
   ],
   "source": [
    "test_set = sorted(read_files(is_training=False), key=lambda sample:sample[0])\n",
    "num_test = len(test_set)\n",
    "print(f'Number of test samples: {num_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_prediction_df = pd.DataFrame(columns=['labels'])\n",
    "test_prediction_df.index.name = 'id'\n",
    "\n",
    "pred_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in range(len(test_set)):\n",
    "        test_sample = test_set[idx]\n",
    "        _id, _age, _sex, _recording = test_sample\n",
    "        age = torch.tensor(_age)\n",
    "        sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n",
    "        recording = torch.tensor(_recording, dtype = torch.float32)\n",
    "\n",
    "        test_in = [recording.unsqueeze(0).to(device), \n",
    "                   torch.reshape(age,(-1,1)).to(device), \n",
    "                   torch.reshape(sex,(-1,1)).to(device)]\n",
    "  \n",
    "        out = model(test_in) \n",
    "        sample_prediction = np.array(torch.sigmoid(out).squeeze().cpu())\n",
    "        pred_list.append(sample_prediction)\n",
    "  \n",
    "th_arr = np.array([0.25, 0.25, 0.25, 0.35, 0.15, 0.15, 0.25, 0.5, 0.4, 0.55, 0.25, 0.1])\n",
    "\n",
    "test_pred = np.array(pred_list) > th_arr\n",
    "\n",
    "for idx in range(len(test_pred)):\n",
    "    test_prediction = test_pred[idx]\n",
    "    indices_of_1s = np.where(test_prediction)[0]\n",
    "    str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n",
    "    test_prediction_df.loc[idx] = [str_indices_of_1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:38:38.301246Z",
     "iopub.status.busy": "2021-05-14T11:38:38.300546Z",
     "iopub.status.idle": "2021-05-14T11:39:17.295789Z",
     "shell.execute_reply": "2021-05-14T11:39:17.296466Z"
    },
    "papermill": {
     "duration": 39.0374,
     "end_time": "2021-05-14T11:39:17.296645",
     "exception": false,
     "start_time": "2021-05-14T11:38:38.259245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "\n",
    "# test_prediction_df = pd.DataFrame(columns=['labels'])\n",
    "# test_prediction_df.index.name = 'id'\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for idx in range(len(test_set)):\n",
    "#         test_sample = test_set[idx]\n",
    "#         _id, _age, _sex, _recording = test_sample\n",
    "#         age = torch.tensor(_age)\n",
    "#         sex = torch.tensor(0) if _sex == \"F\" else torch.tensor(1)\n",
    "#         recording = torch.tensor(_recording, dtype = torch.float32)\n",
    "\n",
    "#         test_in = [recording.unsqueeze(0).to(device), \n",
    "#                    torch.reshape(age,(-1,1)).to(device), \n",
    "#                    torch.reshape(sex,(-1,1)).to(device)]\n",
    "  \n",
    "#         out = model(test_in) \n",
    "#         sample_prediction = torch.sigmoid(out).squeeze() > 0.21039 # Use 0.5 as a threshold / squeeze는 batch dimension을 제거해주기 위함\n",
    "#         indices_of_1s = np.where(sample_prediction.cpu())[0]\n",
    "#         str_indices_of_1s = ' '.join(map(str, indices_of_1s))\n",
    "#         test_prediction_df.loc[idx] = [str_indices_of_1s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:39:17.359758Z",
     "iopub.status.busy": "2021-05-14T11:39:17.358870Z",
     "iopub.status.idle": "2021-05-14T11:39:17.374238Z",
     "shell.execute_reply": "2021-05-14T11:39:17.374734Z"
    },
    "papermill": {
     "duration": 0.048352,
     "end_time": "2021-05-14T11:39:17.374919",
     "exception": false,
     "start_time": "2021-05-14T11:39:17.326567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9 10 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7 10 11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2 3 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3 8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8 10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8 10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels\n",
       "id         \n",
       "0   9 10 11\n",
       "1       3 8\n",
       "2   7 10 11\n",
       "3    2 3 10\n",
       "4         8\n",
       "5      8 10\n",
       "6       3 8\n",
       "7      8 10\n",
       "8         8\n",
       "9      8 10"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prediction_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-14T11:39:17.440968Z",
     "iopub.status.busy": "2021-05-14T11:39:17.440006Z",
     "iopub.status.idle": "2021-05-14T11:39:17.465484Z",
     "shell.execute_reply": "2021-05-14T11:39:17.464990Z"
    },
    "papermill": {
     "duration": 0.059779,
     "end_time": "2021-05-14T11:39:17.465630",
     "exception": false,
     "start_time": "2021-05-14T11:39:17.405851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction_df.to_csv('Chosen_12_threshold.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1813.513466,
   "end_time": "2021-05-14T11:39:18.997586",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-05-14T11:09:05.484120",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
